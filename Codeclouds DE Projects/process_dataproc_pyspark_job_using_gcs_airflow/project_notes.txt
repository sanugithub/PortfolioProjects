1. automated a workflow using Apache Airflow to process daily incoming CSV files from a GCP bucket using a Dataproc PySpark job and saving the transformed data into a Hive table.
2. Used GCP services- DataProc/GCS and Airflow to schedule the dag job.
3. created a bucket called ‘airflow_assmt1’ and placed employee.csv under input_files folder. This file will be picked up by the spark job which is a part of the DAG. Also made sure to place the pyspark job ‘employee_batch.py’ in the python_file folder in the same GCS bucket.
4. I created an airflow cluster and then proceeded to place the ‘airflow_ass1_job.py’ file in the DAG list so that it can be picked up by airflow.
5. Now the various stages can be seen in the DAG. A file sensor checks every 5 mins in the input_file location, once it detects employee.csv a new cluster was created followed by which the spark job was launched which then filters employees with salary >=60,000 and then placing the output in another GCS location in the bucket airflow_assmt1 under the hive_data folder.
6. Then we were able to see that all stages have run successfully followed by deletion of the cluster as well.
7. We can see the resultant data saved in a parquet file format saved in hive_data folder (over which we were able to build external hive tables to query the data)


Challenges:
1. Issues with the spark job being picked up since we had to define a location to save the output.
2. We can’t put the output on the local of the cluster since with the deletion phase of the cluster this data would disappear as well. Hence it makes sense to place the output in a GCS bucket
3. Make sure to specify the format like “hive”/”parquet” while saving the output.