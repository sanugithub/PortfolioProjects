1. created two separate folders in s3 bucket- one for daily raw where we have flights partitioned data and other for dimension table airport data.
2. here flights data acting as fact on and airports data as dimension table
3. Fact table contains quantitative data and primary keys for referenced dimension tables.
4. Dimension tables contain descriptive information. Like here its containing information for each airport
5. On the redshift warehouse, we created 2 tables. One is redshift dimension table 'airports_dim' where we load data from s3 bucket dimension data 'dims' folder into 'airports_dim' redshift table. Other is redhsift fact table 'daily_flights_fact' as destination table.
6. next we create crawlers over s3 daily raw flights data, redshift dimension table and redshift destination table which will be creating glue catalog metadata tables.
7. we used an visual ETL job 'airline_data_ingestion' where we start reading daily raw data received from glue catalog table 'daily_raw'. We also parallely reading glue catalog dimension table. In the next transformation we performed joining of these two tables and following this joining result we changed the schema same matching with destination redshift table. Once 'change schema' part is done, we used to write the final output to the redshift destination table 'dev_airlines_daily_flights'. We also kept 'Job Bookmark' enabled to receive only new or updated data.
8. Further we created 'airline-ingestion-stepfunction' step function to orchestrate multiple steps in your application workflows. As workflow executes, Step Functions tracks which step is being performed and which data is passed between steps. In case of network failure or any other we were able to check that at which point it failed.
9. So on the success step function execution, we also set SNS mail notification in the workflow their to send success alert. Hence, to perform this we set up step function role access to 'AmazonSNSFullAccess' alongwith some other service permission too such as 'CloudwatchDeliveryFullAccess', glue all policy.
10. We configured CloudTrail data events to log/records Amazon S3 bucket API activity. Here S3 events getting passed to cloudtrail and we are receiving API call via Cloudtrail while setting up event bridge rule pattern. So following created an event bridge rule 'airline-stepfunction-trigger' with a custom event pattern to trigger our create step function. An event pattern is defined in json format where we are passing bucket name and file name as suffix in the 'requestParameters'. Next we select step function to trigger as target with event bridge role access to step function.
11. That's how we will be getting success notification on success ETL job execution.