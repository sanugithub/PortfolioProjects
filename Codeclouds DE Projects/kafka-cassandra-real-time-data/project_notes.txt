1. created a real-time data pipeline for processing e-commerce data using Apache Kafka and Apache Cassandra. I was ingesting data from a CSV file using a Kafka producer, transforming the data using a Kafka consumer, and finally storing the processed data in a Cassandra table.
2. So first loaded the 'olist_orders_dataset.csv' into a pandas dataframe and examined its structure and contents.
3. configured Confluent Kafka & created a Kafka topic, named 'ecommerce-orders', to hold the e-commerce data.
4. developed a kafka idempotence producer(idempotence to prevent duplicates) in python that reads the data from the pandas dataframe and publishes it to the 'ecommerce-orders' Kafka topic. The key for each message was a combination of the 'customer_id' and 'order_id' fields from the dataset.
5. Set up Datastax Cassandra & created a keyspace, named 'ecommerce', for storing the e-commerce data. Then designed a table, named 'orders', within the 'ecommerce' keyspace. This table reflected the schema of the incoming data and included additional columns for the derived features: 'OrderHour' and 'OrderDayOfWeek'. The data model had 'customer_id' as the partition key and 'order_id' and 'order_purchase_timestamp' as clustering keys.
6. Developed a Kafka consumer (with having a consumer group) in Python that subscribes to the 'ecommerce-orders' topic. The consumer derived two new columns 'PurchaseHour'/'OrderHour' and 'PurchaseDayOfWeek'/'OrderDayOfWeek', then ingesting transformed data into the 'orders' table in Cassandra.
7. While inserting data into the Cassandra 'orders' table, ensured that the write operations maintain quorum consistency.
8. Tested my data pipeline end-to-end. Ran my Kafka producer to ingest the data, then executed the Kafka consumer to process the data and inserted it into the Cassandra table. Verified the data in the Cassandra table matches the processed data and that all transformations have been executed correctly.