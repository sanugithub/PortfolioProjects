1. note down the gcp project id for 'big-data-projects'.
2. We needed 3 pub sub topics for this project. Topics are orders_data, payments_data and dlq_payments_data.
3. While creating topics, a default subscription gets created for the respective topics.
4. first we run producer of orders data so that some records might be able to ingest into cassandra table. Here attributes related to payment will be null when we check cassandra table.
5. Then we start the payments producer so that it can push the data related to the respective order ids. Next ingestion fact consumer(payments consumer) starts reading data from payments topic so that it can check order id existence in cassandra table and update the payment data for that order id there at table in real time. If order id not found, those data thrown to dlq payments topic. We wrote another streaming process on dlq topic to reprocess dlq data in cassandra.
6. Basically thats how we can handle failure data which was somehow didn't ingest into table.