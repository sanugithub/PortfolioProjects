1. create s3 bucket with bad records, historical_data_rule_outcome, input, rule_outcome
2. created crawler on s3 input source where we also set a data quality rule to monitor the quality of your data assets
3. set a data quality rule to check their must be an imdb rating(not blank). Another rule is rating should consist between 8.5 and 10.3. On running we get passed and failed rule outcomes which was getting stored in s3 bucket another folder 'historical_data_rule_outcome'
4. Created a jdbc connection for connecting crawler to redshift target table. That crawler will be crawling the target redshift table where output results will be loaded
5. build an etl job where s3 input source being read from data catalog. Then we add our first transformation to include our data quality rule set where I also enabled 'add new columns on data quality errors'.
6. We further do 2 more transformation- rowLevelOutcomes and ruleOutcomes
7. ruleOutcomes will be having rule wise outcome with pass or fail reason. And we were storing those in json format at s3 bucket another folder 'rule_outcome'.
8. On the other hand, rowLevelOutcomes will be having row wise outcome with pass or fail reason.
9. Following rowLevelOutcomes, we further next added a conditional router where we checked with filter condition whether 'DataQualityEvaluationResult' matches 'Passed' value or not.
10. once we have groups on failed and passed records group. For failed records, we were storing data in json format at s3 bucket 'bad_records' folder. For passed records, first we were applying 'change schema' transform where we matched both the source key and target key(redshift target table) column's data types and also dropping the 'data quality' additional columns.
11. we were then uploading passed records to the target redshift table where assigned 'IAM' access role with the redshift cluster
12. added vpc endpoints for glue and cloudwatch under same vpc id which is included in redshift cluster
13. we already created a target redshift table 'imdb_movies_rating'
14. we next made an eventbridge rule where a pattern is defined to check receiving event's data quality status using 'AWS Glue Data Quality' service with event type 'Data Quality Evaluation Results Available'. We then invoke a target SNS topic service. Using SNS service we sending success/fail notification.