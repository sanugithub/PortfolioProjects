1. In this mini project, I builded a Kafka producer and a consumer group that worked with a MySQL database, Avro serialization, and multi-partition Kafka topics. The producer was fetching incremental data from a MySQL table and writing Avro serialized data into a Kafka topic. The consumers then deserializing this data and appending it to separate JSON files.
2. MySQL database storing product information such as product ID, name, category, price, and updated timestamp. Scenario is like I was updating the database frequently with new products and changing product information. I wanted to build a real-time system to stream these updates incrementally to a downstream system for real-time analytics.

3. Created a table named 'product' in MySQL database
4. Wrote a Kafka producer in Python that used a MySQL connector to fetch data from the MySQL table. In my producer code, maintained a record of the last read timestamp. Each time I fetched data, used a SQL query to get records where the last_updated timestamp is greater than the last read timestamp.
5. Serialized the data into Avro format and published the data to a Kafka topic named "product_updates". Configured this topic with 10 partitions. Used the product ID as the key when producing messages. This ensured that all updates for the same product end up in the same partition.
6. Updated the last read timestamp after each successful fetch. 

7. Wrote a Kafka consumer in Python and set it up as a consumer group of 5 consumers. Each consumer read data from the "product_updates" topic. Deserialized the Avro data back into a Python object & implemented data transformation logic.
8. Each consumer converting the transformed Python object into a JSON string and appending the JSON string to a separate JSON file. Opened the file in append mode
9. Ensured each new record was being written in a new line for ease of reading.